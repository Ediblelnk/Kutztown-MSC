# Functionalism

## What is a function?

Example: Thermostat
Other Examples?
    Distinguish:
        The function of X (what job X performs)
        What arrangements enable something to perform its function

### Examples of functions

Artifacts like mousetraps, can openers, computers, pin setter at bowling alley, coke machine...
Mathematical functions like x^4 = y
Jobs like mayor, doctor, and plumber

## Mind is a function

Distinguish:
    The function of X, i.e. 'what job X performs'
    What arrangements enable something to perform its function

## Functions

Functions can be described abstractly and completely independent of whatever it is that enables the function to be discharged.

> Input > Internal states > output.

Functions are multiply realizable. Assume that functions must be embodied.

## Mental States as Functional States

"Metaphysical Functionalism" -- Abstract description of a function.

Example: pain (p. 124)
    Input (tissue damage) > Pain > Output (groaning & desire to be rid of pain)

Note that "desire" is another functional state

## Psychofunctionalism

The set of arrangements enable the mental function (abstractly or formally specified) to be executed.

In principle, the functions could be executed in non-physical processes (soul stuff). Most functionalists are materialists. Mental functions are embodied in neurophysiological processes of the pain and nervous system. However, some functionalists maintain that mental functions can be embodied in computer chips.

### Psychofunctionalism versus Identity Theory

Identity theory tries to identify specific types of mental states with specific types of brain states. But, the multi-realizability of mental states makes this implausible.

Psychofunctionalism does not have this problem, since it does not try to reduce or identify types of mental states with types of brain states. A mental state such as pain could be realized in a variety of physical arrangements and, in principle, non-physical arrangements.

## Computational or Turing Machine Functionalism

See pages 129-134, see Ned Block's example of Coke Machine

Kathleen Wilkes' washing machine (hierarchically structured with subsystems)

## Machine Functionalism

Each mental system is described by at least one Turing Machine Table (TM) of a certain sort, and each type of mental state is identical to one of the machine table states.

Example: Machine table for a Coke machine. The TM itself is an abstract automation. The table is a description of the TM.

### Machine Table for Coke Machine

| | S1 = Dime desire | S2 = Nickel desire |
| :-: | :-: | :-: |
| **Nickel input** | Emit no output, Go to S2 | Emit a Coke, Go to S1 |
| **Dime input** | Emit a Coke, Stay in S1 | Emit a Coke and Nickel, Go to S1 |

If we considered the Coke machine a mental system, it would consists of two mental states:
    S1 = dime desire,
    S2 = nickel desire.

The table shows the specific relationship between the inputs, outputs, and states of the system. It indicates two functions:

1. The transitions from **inputs and states to *outputs***. For example, if the machine is in S1, and we put a dime into the machine, the machine puts out a Coke and remains the state S1.

1. The transition from **inputs and states to *states***. For example, if the machine is in S1 and we put in a nickel, the machine emits *no output* but goes into S2. Note S1 is part of the causal story of the machine going into S2. Mental states are individuated partly in terms of causal relations to other mental states.

There are four conditions to which the Coke machine table is equivalent to or representative of.

Any system that has a set of inputs, outputs, and states related in the way specified by the machine table is "**described**" by the machine table and is a "**realization**" of the abstract automation specified by the machine table.

The abstract automation or Turing machine can be realized by other machines, i.e. a 7-Up machine constructed of different material than that of the Coke machine, as long as the inputs, outputs, and states are related in the way specified by the machine table.

We can use the machine table to describe any system that has inputs, outputs, and internal states in the way specified by the machine table.

**Functionalists claim that** *in the same way that we can describe what it is for a simple system to be in state S1 or S2 in terms of the machine table, we can describe for any complex system that has inputs, outputs, and internal states what it is for that system to be in a particular state by a complex machine table.*

## Putnam's Qualifications

1. When applied to an organism, we must introduce the notion of a probabilistic automation: a Turing machine with probabilistic transitions between states rather than deterministic ones.

1. We must allow for the probabilistic automation to have sensory inputs, and motor-behavioral outputs. (Needed for TM as a model for mental operations of an organism.)

## Functionalism as a Theory of Mind

Human mind is an enormously complex machine table incarnated in the neurophysiological processes of the brain.

Human beings receive inputs in the form of sensory and perceptual information. This information is then processed in the brain. Then there is output in the form of behavior.

What makes any mental state the mental state that it is (what makes a pain a pain) is its having a certain causal role in relation to inputs, outputs, and other mental states.

## What Goes On In Our Brains?

Kathleen Wilkes' washing machine (hierarchically structured with subsystems)

Simplest functions are realized in molecules and atoms that make up nerve cells. Higher functions realized in brain structures like hippocampus, which is linked to long-term memories or Broca's area, which is linked to language processing.

Functionalism is an information processing model of the mind.

## Strength's of Functionalism

1. **Avoids behaviorism's error** of identifying mental states with behavior or potential outward behavior and also identity theory's error of claim that mental states can only exist as states of human brains.

1. **May fit with common sense idea that states like pain result from input** (tissue damage) and causes pain behavior and desire to be rid of pain.

1. Avoids problem of mind/body causal interaction, since mental states are defined in terms of the entire function of inputs, internal transition states, and outputs.

## Problems for Functionalism

**Liberalism**: Functionalism attributes mental states to things that (many say) don't have them.

Functionalism cannot account for:

1. **Qualia** - the subjectivity and privacy of mental states like sensations.
1. **Intentionality** - that mental states like beliefs, desires, and emotions are *about* certain things

### Block's Objection to Functionalism

Functionalism is either too liberal because it attributes mental states to systems that do not have them or chauvinistic because it withholds mental states from systems that have them.

Liberalism is shown by:
    Homunculi-Headed Robots
    The Chinese Mind

### Homunculi-head

Imagine a body externally like a human body, say yours, but internally quite different. The neurons from sensory organs are connected to a bank of lights in a hollow cavity in the head. A set of buttons connects to the motor-output neurons. Inside the cavity resides a group of little men. Each has a very simple task: to implement a "square" of an adequate machine table that describes you. On one wall is a bulletin board on which is posted a state card, i.e. a card that bears a symbol designating one of the states specified in the machine table.

Here is what the little men do: Suppose the posted card has a "G" on it... Suppose the light representing input "I17" goes on. One of the G-men has the following as his sole task: when the card reads "G" and the "I17" light goes on, he pressed the button "O191" and changes the state card to "M"... In spite of the low level of intelligence required of each little man, **the system as a whole manages to simulate you because the functional organization they have been trained to realize is yours**... (Block, p. 278)

### The Chinese Mind

Suppose we convert the government of China to functionalism, and we convince its officials to realize a human mind for an hour. We provide each of the billion people in China--chosen because of its billion inhabitants--with a specially designed two-way radio that connects them in the appropriate way to other persons to recreate the artificial body mentioned in the previous example.

We replace each of the little men with a citizen of China plus his radio. Instead of a bulletin board, we arrange to have letters displayed on a series of satellites placed so that they can be seen from anywhere in China.

The system of a billion people communicating with one another plus satellites plays the role of an external "brain" connected to the artificial body by radio...

It is not obvious that the China-body system is physically impossible. It could be functionally equivalent to you for a short time, say an hour. (Block)

### Block

> "In describing the Chinese system as a Turing Machine, I have drawn the line in such a way that it satisfies a certain type of functional description--one that you also satisfy, and one that, according to functionalism, justifies attributions to mentality."

As described, the Chine Mind has the same functional organization as someone and so if functionalism entails ascribing mental states to someone, it entails ascribing mental states to the Chine Mind.

## Block's Absent Qualia Argument Against Functionalism

Do the HH-Robot and China-Mind have mental states? Would they experience things like pains, raw feels, and the qualia of perceptions?

### Putnam's Response

Putnam stipulates that a pain-feeling organism must have a functional organization but has no parts which (1) themselves possess that sort of functional organization and also (2) play a crucial role in giving the whole system its functional organization.

Block's response: Block objects that this stipulation is *ad hoc* and too strong as it rules out the possibility of beings who intuitively *would* share all of our mental states.

Block offers a hypothetical case in support of his claim. Suppose there are tiny intelligent creatures who are smaller than our elementary particles. They build spaceships that mimic the behavior of our elementary particles, like oxygen and carbon. Imagine that we visit the planets of these creatures and then eat food made up of them and their spaceships, and gradually come to be made of "matter" composed of the tiny creatures in space ships.

According to Putnam, this should make a difference in our mental lives. But that seems to be the wrong conclusion. Why should the fact that you are now made of matter that contains parts that themselves possess functional organization and that play a crucial role in our functional organization make any difference to what sort of mental life we would have?

### Psychofunctionalist Response

Psychofunctionalism specifies inputs and outputs in terms of species-specific neural activity.

Can functionalists appeal to functional differences between the cognitive mechanisms in our brains and the mechanisms in the Homunculi-head and China-brain to rule our mentality in the Homunculi-head and China-brain?

Block says no.

## Block's Problem of Inverted Qualia

Couldn't two people be in psychofunctionally equivalent in terms of inputs and outputs but differ in their mental states.

E.g., Two people may both say that the tree is green, even though what looks green to one person may look red to another.
E.g., Couldn't two people be in the same functional state but one feels pain and the other does not?

Note: Functionalism may do better with non-qualitative mental states like beliefs and desires, since it hard to make sense of how two individuals with opposite beliefs or desires could be functionally equivalent, as the difference could reveal itself in some possible behavior.

## Psychofunctionalism is Chauvinistic

- Chauvinism denies mental states to things that intuitively have them.
- Imagine Martians who appear the same as us in all functional ways but differ in terms of their underlying cognitive mechanisms

> "We develop extensive cultural and commercial intercourse with [the Martians]. We study each other's science and philosophy journals, go to each other's movies, read each other's novels, etc. Then Martian and Earthian psychologists compare notes, only to find that in underlying psychology, Martians and Earthians are very different... Imagine that what Martian and Earthians find when they compare notes is that Martians and Earthians differ as if they were the end products of maximally different design choices (compatible with rough functional equivalence in adults.) Should we reject our assumption that Martians can enjoy our films, believe their own apparent scientific results, etc?... Surely there are many ways of filling in the Martian/Earthian difference I sketched on which it would be perfectly clear that even if Martians behave differently from us on subtle psychological experiments, they nonetheless think, desire, enjoy, etc. To suppose otherwise would be crude human chauvinism."

### What to do?

Can we consider the domain of psychology to include creatures with mentality, including Martians who are not psychofunctionally equivalent to us?

Can we define "psychofunctionalism" in terms of "universal" or "cross-system" psychology, rather than human psychology?

Can we develop a science of mentality that would encompass all creatures with mentality regardless of the functional mechanisms in which mentality is realized?

Summary of argument at end of Section 3.0

## Block: Additional Problems for Functionalism

1. **Common sense functionalists** specify input in terms of sensory input and output in terms of behavior and mental states in terms of their causal relations to such inputs and output. But then no system without such input and output could have mental states. **Do we really want to say that creatures without our type of sensory input and output could not have mental states?**
1. **Psychofunctionalists** specify input and output in terms of human neural activity and mental states in terms of their causal relations to such inputs and output. But then no system without such neural input and output could have the kind of mental states that we have. **Do we really want to say that creatures without our type of neural input and output could not have the kind of mental states that we have?**
1. Can functionalists characterize inputs and outputs in more abstract terms and leave open what the inputs and outputs are intrinsically like? So, two systems would be functionally equivalent if they had isomorphic inputs, outputs, and internal states that were causally related to those inputs and outputs? **Block**: No. Such a version of functionalism would then be wildly liberal.

### Reductio ad Absurdum

**Block's example**: Economic systems have inputs and outputs, i.e. influx and outflux of credits and debits. And economic systems also have a rich variety of internal states, i.e. having a rate of increase of GNP equal to double the Prime Rate. It does not seem impossible that a wealthy sheik could gain control of the economy of a small country, i.e. Bolivia, and manipulate its financial system to make it functionally equivalent to a person, i.e. himself. If this seems implausible, remember that the economic states, inputs, and outputs designated by the sheik to correspond to his mental states, inputs, and outputs, need not be "natural" economic magnitudes... The mapping from psychological magnitudes to economic magnitudes could be as bizarre as the sheik requires.

**Block**: Clearly, the economy of Bolivia cannot have any mental states.
